[["index.html", "Portfolio bookdown 1 Introduction to my portfolio", " Portfolio bookdown Abdurrahman Uyar 1 Introduction to my portfolio This is my portfolio showcasing skills I have acquired during a course “Workflows” in a data sciences for biology track in the form of a bookdown. "],["effects-of-different-compounds-on-c.elegans-offspring.html", "2 Effects of different compounds on C.elegans offspring", " 2 Effects of different compounds on C.elegans offspring In this report I am trying to plot a dataset of different compounds on C. elegans offspring. This report is for showing my skills in R that I have acquired during courses in a data science for biology track. To start off, I am loading the libraries that are used and reading the excel data into R. # Loading libraries library(tidyverse) library(readxl) library(here) # Reading data into R data &lt;- read_excel(here(path = &quot;Data/CE.LIQ.FLOW.062_Tidydata.xlsx&quot;)) # Checking datatypes and variables. str(data) After checking the datatypes of variables RawData, compName and compConcentration, these have numeric, character and again character as datatypes respectively. compConcentration should be numeric, or the plot would look like this: data %&gt;% ggplot(aes(x = compConcentration, y = RawData, colour = compName, shape = expType)) + geom_point() + theme_bw() + theme(axis.text.x = element_text(angle = 90)) Because variable compConcentration is of character datatype, it is not plotted in the correct order. # Changing datatypes data$compName &lt;- as.factor(data$compName) data$compConcentration &lt;- as.numeric(data$compConcentration) # Making data a bit more readable by selecting fewer columns data2 &lt;- data %&gt;% select(vialNr, dropCode, expType, expReplicate, RawData, compName, compConcentration, compUnit, compVehicle) # Log10 of compConcentration and adding geom_jitter + geom_smooth to make plot more readable. data2 %&gt;% ggplot(aes(x = log10(compConcentration), y = RawData, colour = compName, shape = expType)) + theme_bw() + geom_jitter(width = 0.2) + labs(title = &quot;Offspring C. elegans after incubation with decane, napthalene, \\n2,6-diisopropylnapthalene, S-medium or ethanol&quot;, x = &quot;Log10 concentration component&quot;, y = &quot;Offspring amount&quot;) Compared to the previous plot, here the variable compConcentration is converted to numeric (and also taken the log10 of). Geom_jitter is also used so datapoints don’t overlap too much. The positive control in this experiment is ethanol, and the negative control is S-medium # Take the average offspring per compound, per concentration summary_data &lt;- data2 %&gt;% group_by(compName, compConcentration,expType) %&gt;% summarise(gem = mean(RawData, na.rm = T)) # Take the average offspring count of the negative control nc_gem &lt;- summary_data$gem[summary_data$expType == &quot;controlNegative&quot;] # Make a new column with normalised data, normalised by taking the fraction relative to the negative control, where the nc equals 1.0 summary_data &lt;- summary_data %&gt;% mutate(fractie = gem/nc_gem) # Filtering out positive/negative controls, so they don&#39;t show up in the plot. summary_plotting &lt;- summary_data %&gt;% filter(compName == &quot;decane&quot; | compName == &quot;2,6-diisopropylnaphthalene&quot; | compName == &quot;naphthalene&quot;) summary_plotting %&gt;% ggplot(aes(x = log10(compConcentration), y = fractie, colour = compName)) + geom_point() + geom_smooth() + theme_bw() + ylim(0,1.3) + labs(title = &quot;Offspring C. elegans after treatment with decane, napthalene \\nor 2,6-diisopropylnaphthalene&quot;, x = &quot;Log10 component concentration&quot;, y = &quot;Fraction offspring (relative to, negative control)&quot;) + guides(color = guide_legend(&quot;Component&quot;)) + scale_colour_manual(values = c(&quot;red&quot;,&quot;blue&quot;,&quot;darkgreen&quot;)) In this plot the data is normalised relative to the negative control (S-medium), where offspring originating from C. elegans incubated in S-medium are equal to 1.0 Normalisation is done so the different compounds could be easily compared to the negative control, for which we know shouldn’t have an effect on offspring. If we would look at if there is an actual statistical significant difference in offspring count for the different compound incubations, we would perform the following steps: 1: Shapiro wilk test on the different groups, to see if the data is normally distributed 2: If the data is normally distributed, also perform a Levene’s test, because the experimental design is unpaired. 3: Then perform unpaired T-tests. "],["transparency-criteria.html", "3 Transparency criteria", " 3 Transparency criteria In this report I am doing an open peer review of a scientific paper by filling in transparency criteria, I am also reproducing a plot in a paper to test reproducibility of the code. This is the first article I have chosen: Article: Epidemiological and evolutionary consequences of different types of CRISPR-Cas systems Authors: Hélène Chabas, Viktor Müller, Sebastian Bonhoeffer and Roland R. Regoes The general aim of this study was to determine the importance of molecular differences between 3 types of CRISPR-cas systems. The authors developed a stochastic epidemiological model for this. Outcome would be that type 3 systems could outcompete type 1 and 2. This is because for type 1 and 2 the control of phage evolution is mediated by spacer diversity and there is a diversity critical threshold whereas for type 3 CRISPR-Cas systems, spacer diversity does not impact phage extinction and the epidemiological outcome is driven by the probability to generate at least one resistant genotype. library(tibble) score_table &lt;- tibble(`Transparency criteria` = c(&quot;Study purpose&quot;, &quot;Data availability statement&quot;, &quot;Data location&quot;, &quot;Study location&quot;, &quot;Author review&quot;, &quot;Ethics statement&quot;, &quot;Funding statement&quot;, &quot;Code availability&quot;), Score = c(&quot;Yes&quot;, &quot;Yes&quot;, &quot;Data location not stated&quot;, &quot;No&quot;, &quot;Tier 4&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;) ) knitr::kable(score_table) Transparency criteria Score Study purpose Yes Data availability statement Yes Data location Data location not stated Study location No Author review Tier 4 Ethics statement No Funding statement Yes Code availability Yes For trying to reproduce a figure I’ve decided to use the following article: Title: Monitoring trends and differences in COVID-19 case fatality rates using decomposition methods: Contributions of age structure and age-specific fatality. Authors: Christian Dudel Tim Riffe Enrique Acosta Alyson van Raalte Cosmo Strozza Mikko Myrskylä The code in this paper is showing how the death rate progresses as COVID-19 cases rise per country and New York City In terms of readability of the code, I would grade it a 3 out of 5. This is because the filtering steps are harder to read for me, but the plotting itself is very readable. If I had to scale how easy the plot is to reproduce where 1 is very hard, and 5 is very easy, I’d grade it a 5. This is because the only thing that had to be changed was the path for the inputfile. ### Monitoring trends and differences in COVID-19 case fatality ############## ### rates using decomposition methods: A demographic perspective ############## ### Last updated: 2020-07-22 11:18:52 CEST ### Contact: ### riffe@demogr.mpg.de ### acosta@demogr.mpg.de ### dudel@demogr.mpg.de ### Packages ################################################################## library(tidyverse) library(ggrepel) library(scales) library(here) ### Load data ################################################################# # Load data db_gh &lt;- read_csv(here(&quot;Data/inputdata.csv&quot;)) view(db_gh) ### Aggregate data ############################################################ # Filter date db_gh$Date &lt;- as.Date(db_gh$Date,&quot;%d.%m.%y&quot;) db_gh2 &lt;- db_gh %&gt;% filter(Date&lt;=as.Date(&quot;30.06.2020&quot;,&quot;%d.%m.%y&quot;)) # Set New York as &quot;country&quot; (easier handling) db_gh2$Country[db_gh2$Country==&quot;USA&quot; &amp; db_gh2$Region == &quot;NYC&quot;] &lt;- &quot;NYC&quot; # Sum data over age groups db_gh2 &lt;- db_gh2 %&gt;% filter(!Country %in% c(&quot;China&quot;,&quot;USA&quot;,&quot;South Korea&quot;) &amp; Sex == &quot;b&quot;) %&gt;% group_by(Country, Code,Date) %&gt;% summarise(Cases = sum(Cases), Deaths = sum(Deaths)) # Exclude bolletino db_gh2 &lt;- db_gh2 %&gt;% filter(str_sub(Code, 1, 5) != &quot;ITbol&quot;) # Sort by date db_gh2 &lt;- db_gh2 %&gt;% group_by(Country) %&gt;% arrange(Date) # Smooth reporting issues cases for(country in unique(db_gh2$Country)) { days &lt;- db_gh2$Date[db_gh2$Country==country] for(day in 2:length(days)) { current &lt;- db_gh2$Cases[db_gh2$Country==country &amp; db_gh2$Date==days[day]] previous &lt;- db_gh2$Cases[db_gh2$Country==country &amp; db_gh2$Date==days[day-1]] if(current&lt;previous) db_gh2$Cases[db_gh2$Country==country &amp; db_gh2$Date==days[day]] &lt;- previous } } # Smooth reporting issues deaths for(country in unique(db_gh2$Country)) { days &lt;- db_gh2$Date[db_gh2$Country==country] for(day in 2:length(days)) { current &lt;- db_gh2$Deaths[db_gh2$Country==country &amp; db_gh2$Date==days[day]] previous &lt;- db_gh2$Deaths[db_gh2$Country==country &amp; db_gh2$Date==days[day-1]] if(current&lt;previous) db_gh2$Deaths[db_gh2$Country==country &amp; db_gh2$Date==days[day]] &lt;- previous } } ### Plot settings ############################################################# # Set colors col_country &lt;- c(&quot;Germany&quot; = &quot;black&quot;, &quot;Italy&quot; = &quot;#2ca25f&quot;, &quot;NYC&quot;=&quot;#f0027f&quot;, &quot;Spain&quot;=&quot;#beaed4&quot;, &quot;South Korea&quot;=&quot;#fdc086&quot;)#, #&quot;USA&quot;=&quot;#386cb0&quot;) cols &lt;- c(&quot;black&quot;, &quot;#2ca25f&quot;, &quot;#f0027f&quot;, &quot;#beaed4&quot;, &quot;#fdc086&quot;)#, #&quot;#386cb0&quot;) # Axis labs &lt;- db_gh2 %&gt;% group_by(Country) %&gt;% filter(Cases == max(Cases)) %&gt;% mutate(Cases = Cases + 3000) # Including all reports tx &lt;- 6 lim_x &lt;- 240000 ### Plot ###################################################################### db_gh2 %&gt;% ggplot(aes(Cases, Deaths, col = Country))+ geom_line(size = 1, alpha = .9)+ scale_x_continuous(expand = c(0,0), breaks = seq(0, 300000, 50000), limits = c(0, lim_x + 30000), labels = comma)+ scale_y_continuous(expand = c(0,0), breaks = seq(0, 40000, 5000), limits = c(0, 40000), labels = comma)+ annotate(&quot;segment&quot;, x = 0, y = 0, xend = lim_x, yend = lim_x * .02, colour = &quot;grey40&quot;, size = .5, alpha = .3, linetype = 2)+ annotate(&quot;segment&quot;, x = 0, y = 0, xend = lim_x, yend = lim_x * .05, colour = &quot;grey40&quot;, size = .5, alpha = .3, linetype = 2)+ annotate(&quot;segment&quot;, x = 0, y = 0, xend = lim_x, yend = lim_x * .10, colour = &quot;grey40&quot;, size = .5, alpha = .3, linetype = 2)+ annotate(&quot;segment&quot;, x = 0, y = 0, xend = lim_x, yend = lim_x * .15, colour = &quot;grey40&quot;, size = .5, alpha = .3, linetype = 2)+ annotate(&quot;text&quot;, label = &quot;2% CFR&quot;, x = lim_x + 1000, y = lim_x * .02, color=&quot;grey30&quot;, size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) + annotate(&quot;text&quot;, label = &quot;5% CFR&quot;, x = lim_x + 1000, y = lim_x * .05, color=&quot;grey30&quot;, size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) + annotate(&quot;text&quot;, label = &quot;10% CFR&quot;, x = lim_x + 1000, y = lim_x * .10, color=&quot;grey30&quot;, size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) + annotate(&quot;text&quot;, label = &quot;15% CFR&quot;, x = lim_x + 1000, y = lim_x * .15, color=&quot;grey30&quot;, size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) + scale_colour_manual(values = cols)+ geom_text(data = labs, aes(Cases, Deaths, label = Country), size = tx * .35, hjust = 0, fontface = &quot;bold&quot;) + theme_classic()+ labs(x = &quot;Cases&quot;, y = &quot;Deaths&quot;)+ theme( panel.grid.minor = element_blank(), legend.position = &quot;none&quot;, plot.margin = margin(5,5,5,5,&quot;mm&quot;), axis.text.x = element_text(size = tx), axis.text.y = element_text(size = tx), axis.title.x = element_text(size = tx + 1), axis.title.y = element_text(size = tx + 1) ) "],["folder-structures.html", "4 Folder structures 4.1 Folder structure of DAUR2: 4.2 Folder structure of workflows (at time of making folder structure):", " 4 Folder structures In this report im showing my folder structure for the courses Data analysis using R part 2, and the course workflows. These courses are from a data science for biology track I have followed. library(png) library(grid) library(gridExtra) library(here) # I have used the fs package for creating these figures, in particular the dir_tree() function. daur2_fs &lt;- rasterGrob(as.raster(readPNG(here(&quot;Figures/DAUR2_fs.png&quot;)))) workflows_fs &lt;- rasterGrob(as.raster(readPNG(here(&quot;Figures/workflows_fs_.png&quot;)))) 4.1 Folder structure of DAUR2: grid.arrange(daur2_fs) 4.2 Folder structure of workflows (at time of making folder structure): grid.arrange(workflows_fs) "],["my-cv-with-vitae-r-package.html", "5 My CV with vitae R package 5.1 Script 5.2 Credits", " 5 My CV with vitae R package 5.1 Script The script I have used for creating this CV can be seen here on my github. 5.2 Credits I have used the vitae package for this. This is the github page of the creator of this R package "],["orientation-for-a-new-skill.html", "6 Orientation for a new skill 6.1 Looking ahead 6.2 First steps 6.3 scRNA analysis beginning steps, using Cell ranger and Seurat 6.4 Credits", " 6 Orientation for a new skill 6.1 Looking ahead In about 2 years, I hope to be working in a laboratory where NGS-experiments are done. A couple weeks ago, I heard back from GenDx (Utrecht, The Netherlands) at which I applied for, for an internship place. Reply was positive and I can start interning starting in September. GenDx is a company where NGS-experiments are done on a regular basis. So in regards to my goal, I think I am doing good. For the workflows course in the track data science for biology we have been assigned around 32 hours to develop a new skill. In one of the courses we were taught how to analyse data from a bulk RNA-seq experiment. A next skill to learn would be how to do the beginning steps of analysing data from a single cell RNA-seq experiment, starting from fastq files. I have made a planning on how to spend my time trying to learn this new skill, which can be seen here: Table 6.1: My planning for learning to do the beginning steps of scRNA-seq analysis Days Action 1 Reading about single-cell RNA-seq, and trying to find tutorials. 2 Finding dataset, loading in the data and downloading the required packages 3 Filter data and obtain count matrix 4 Try basic visualisations 6.2 First steps My first step was reading up on scRNA-seq. I have found a useful course on scRNA-seq that can be found here. According to this course, Cell Ranger is the default tool for processing 10x Genomics scRNAseq data. So this is what I will be using first. Afterwards I am planning on using Seurat to do basic quality control and filtering. I have also found a geo dataset I can use here. 6.3 scRNA analysis beginning steps, using Cell ranger and Seurat (I haven’t ran the bash scripts, because I don’t have a linux OS which is required for Cell ranger, and according to 10x genomics the minimum requirements include 1TB of free disk space and 64GB of RAM, which I don’t have.) # Downloading SRA-toolkit wget --output-document sratoolkit.tar.gz https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-ubuntu64.tar.gz # Extracting content SRA-toolkit tar -vxzf sratoolkit.tar.gz # Downloading the fastq files #!/bin/bash fastq-dump SRR9304760 # Sample: Negative control 1 fastq-dump SRR9304761 fastq-dump SRR9304764 # Sample: Negative control 2 fastq-dump SRR9304765 fastq-dump SRR9304770 # Sample: Negative control 3 fastq-dump SRR9304771 fastq-dump SRR9304772 fastq-dump SRR9304773 # Move fastq files to folders sample_NC1, sample_NC2 and sample_NC3 # Downloading Cell ranger curl -o cellranger-7.1.0.tar.gz &quot;https://cf.10xgenomics.com/releases/cell-exp/cellranger-7.1.0.tar.gz?Expires=1686048220&amp;Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZi4xMHhnZW5vbWljcy5jb20vcmVsZWFzZXMvY2VsbC1leHAvY2VsbHJhbmdlci03LjEuMC50YXIuZ3oiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODYwNDgyMjB9fX1dfQ__&amp;Signature=htIYtjO0vxujG1KRKghHR9ztOY4KzeAbIA2zgHeohWdC07N382BFub53plHzSOHX5mQhPxvKMkFuOauGiHqjA5RYBH8oUK1h6Z8uwq4SSpMq5JPOGAZIly5~GwxR6f96W1UOsYIr2ELvxxNGhLtuRDVEsPPfRLWA~JSyhwIzwA1f4AzYGK0CjUbvrM7qjR~fD8rzI7qYzmcwKu9TLTc~MASDkWAo2wAkj5RJJcxn63y~u86MgKfVbQVsBIDa1VQQgudhh4fl0261SvS695taleZJASLwJVmZ6W01iekOX77X2ejb24Ml6pE38gGvpXITtq0DB5cZz1D~-FUXC6Ui-Q__&amp;Key-Pair-Id=APKAI7S6A5RYOXBWRPDA&quot; # Downloading human reference genome (Only using data from human) curl -O https://cf.10xgenomics.com/supp/cell-exp/refdata-gex-GRCh38-2020-A.tar.gz # Extracting Cell ranger tar -xf cellranger-7.1.0.tar.gz # Extracting reference genome tar -xf refdata-gex-GRCh38-2020-A.tar.gz # Using Cell ranger count for making raw count matrix sample nc1 : --id for which folder to output matrix to, --transcriptome is for folder in which the reference genome is in, --fastqs is the folder with the fastq files. --sample is sample prefix, --expect-cells is how many cells the experiment designers aimed for. cellranger-7.1.0/cellranger count --id NML1 --transcriptome refdata-gex-GRCh38-2020-A/ --fastqs sample_nc1/ --sample nc1 --expect-cells 10000 # for sample nc2 cellranger-7.1.0/cellranger count --id NML2 --transcriptome refdata-gex-GRCh38-2020-A/ --fastqs sample_nc2/ --sample nc2 --expect-cells 10000 # for sample nc3 cellranger-7.1.0/cellranger count --id NML3 --transcriptome refdata-gex-GRCh38-2020-A/ --fastqs sample_nc3/ --sample nc3 --expect-cells 10000 At this point I am actually running the code, I have obtained the raw count matrices, the genes and the barcodes that should result from these bash scripts from geo accession number GSE132771. In which, among other things, healthy lung tissue is used. I have only used the data for healthy lung tissue here. library(Seurat) library(tidyverse) library(ggpubr) library(here) # nc stand for negative control nc1 &lt;- Read10X(data.dir = here(&quot;Data/GSE132771_RAW/NML1/&quot;)) nc2 &lt;- Read10X(data.dir = here(&quot;Data/GSE132771_RAW/NML2/&quot;)) nc3 &lt;- Read10X(data.dir = here(&quot;Data/GSE132771_RAW/NML3/&quot;)) # Make Seurat objects of the three datasets, min.cells = 3 so at least 3 cells contain the same gene (feature), min.features is set to 200 so only cells where at least 200 genes were detected are used for the object. nc1 &lt;- CreateSeuratObject(counts = nc1, project = &quot;nc1&quot;, min.cells = 3, min.features = 200) nc2 &lt;- CreateSeuratObject(counts = nc2, project = &quot;nc2&quot;, min.cells = 3, min.features = 200) nc3 &lt;- CreateSeuratObject(counts = nc3, project = &quot;nc3&quot;, min.cells = 3, min.features = 200) # Merge the objects merged_nc &lt;- merge(nc1, y = c(nc2, nc3), add.cell.ids = c(&quot;nc1&quot;,&quot;nc2&quot;,&quot;nc3&quot;), project = &quot;merged_nc&quot; ) # In metadata, store a column for percentage of genes that are mitochondrial genes. merged_nc &lt;- PercentageFeatureSet(merged_nc, pattern = &quot;^MT-&quot;, col.name = &quot;percent.mt&quot;) # And adding percentage ribosomal proteins (names begin with RPS or RPL). merged_nc &lt;- PercentageFeatureSet(merged_nc, pattern = &quot;^RP[SL]&quot;, col.name = &quot;percent.rb&quot;) # Right now, in metadata, each cell has 5 fields. These are: # 1: orig.ident (dataset ID) # 2: nCount_RNA (number of UMI reads detected per cell) # 3: nFeature_RNA (number of expressed genes, that are detected, per cell) # 4: percent.mt (percentage mitochondrial genes) # 5: percent.rb (percentage ribosomal genes) head(merged_nc@meta.data) ## orig.ident nCount_RNA nFeature_RNA percent.mt percent.rb ## nc1_AAACCTGAGTGTTGAA-1 nc1 2252 938 3.0639432 20.825933 ## nc1_AAACCTGCAGCCACCA-1 nc1 9481 2186 1.9934606 24.174665 ## nc1_AAACCTGCATACGCCG-1 nc1 2838 1321 0.8104299 9.126145 ## nc1_AAACCTGCATTTCACT-1 nc1 5110 1933 3.7181996 18.551859 ## nc1_AAACCTGGTGTAACGG-1 nc1 7035 1605 3.7953092 16.972281 ## nc1_AAACGGGAGCTAGTGG-1 nc1 1863 816 6.6559313 38.003221 # Inspecting the ranges of nFeatures_RNA, nCount_RNA and percent.mt range(merged_nc$nFeature_RNA) ## [1] 200 5597 range(merged_nc$nCount_RNA) ## [1] 503 33202 range(merged_nc$percent.mt) ## [1] 0.00000 63.14103 # Visualise nFeature, nCount, percent.mt and percent.rb in violin plot VlnPlot(merged_nc, features = c(&quot;nFeature_RNA&quot;,&quot;nCount_RNA&quot;,&quot;percent.mt&quot;,&quot;percent.rb&quot;), ncol = 4, pt.size = .1) # subsetting each sample subset_nc1 &lt;- subset(merged_nc, idents = &quot;nc1&quot;) subset_nc2 &lt;- subset(merged_nc, idents = &quot;nc2&quot;) subset_nc3 &lt;- subset(merged_nc, idents = &quot;nc3&quot;) # Correlation between nCount_RNA and percent.mt (number on top is Pearson&#39;s r correlation) cor1_1 &lt;- FeatureScatter(subset_nc1, feature1 = &quot;nCount_RNA&quot;, feature2 = &quot;percent.mt&quot;) cor2_1 &lt;- FeatureScatter(subset_nc2, feature1 = &quot;nCount_RNA&quot;, feature2 = &quot;percent.mt&quot;) cor3_1 &lt;- FeatureScatter(subset_nc3, feature1 = &quot;nCount_RNA&quot;, feature2 = &quot;percent.mt&quot;) cor4_1 &lt;- FeatureScatter(merged_nc, feature1 = &quot;nCount_RNA&quot;, feature2 = &quot;percent.mt&quot;) ggarrange(cor1_1, cor2_1, cor3_1, cor4_1) MT percentage negatively correlates with nCount_RNA.(except for nc2). So a higher MT percentage equals a lower UMI-count. (indicates death of a cell) # Correlation between nCount_RNA and nFeature_RNA cor1_2 &lt;- FeatureScatter(subset_nc1, feature1 = &quot;nCount_RNA&quot;, feature2 = &quot;nFeature_RNA&quot;) cor2_2 &lt;- FeatureScatter(subset_nc2, feature1 = &quot;nCount_RNA&quot;, feature2 = &quot;nFeature_RNA&quot;) cor3_2 &lt;- FeatureScatter(subset_nc3, feature1 = &quot;nCount_RNA&quot;, feature2 = &quot;nFeature_RNA&quot;) cor4_2 &lt;- FeatureScatter(merged_nc, feature1 = &quot;nCount_RNA&quot;, feature2 = &quot;nFeature_RNA&quot;) ggarrange(cor1_2, cor2_2, cor3_2, cor4_2) # Correlation between nCount_RNA and percent.rb cor1_3 &lt;- FeatureScatter(subset_nc1, feature1 = &quot;nCount_RNA&quot;, feature2 = &quot;percent.rb&quot;) cor2_3 &lt;- FeatureScatter(subset_nc2, feature1 = &quot;nCount_RNA&quot;, feature2 = &quot;percent.rb&quot;) cor3_3 &lt;- FeatureScatter(subset_nc3, feature1 = &quot;nCount_RNA&quot;, feature2 = &quot;percent.rb&quot;) cor4_3 &lt;- FeatureScatter(merged_nc, feature1 = &quot;nCount_RNA&quot;, feature2 = &quot;percent.rb&quot;) ggarrange(cor1_3,cor2_3,cor3_3,cor4_3) # Correlation between percent.rb and percent.mt cor1_4 &lt;- FeatureScatter(subset_nc1, feature1 = &quot;percent.rb&quot;, feature2 = &quot;percent.mt&quot;) cor2_4 &lt;- FeatureScatter(subset_nc2, feature1 = &quot;percent.rb&quot;, feature2 = &quot;percent.mt&quot;) cor3_4 &lt;- FeatureScatter(subset_nc3, feature1 = &quot;percent.rb&quot;, feature2 = &quot;percent.mt&quot;) cor4_4 &lt;- FeatureScatter(merged_nc, feature1 = &quot;percent.rb&quot;, feature2 = &quot;percent.mt&quot;) ggarrange(cor1_4,cor2_4,cor3_4,cor4_4) It seems that MT percentage has a slightly negative correlation with percent.rb (except for nc2). This would indicate that high ribosomal protein content seems to contain biological signal # Getting rid of low quantity cells merged_nc &lt;- subset(merged_nc, subset = nFeature_RNA &gt; 500 &amp; nFeature_RNA &lt; 4000 &amp; nCount_RNA &lt; 20000 &amp; percent.mt &lt; 10) # Visualise again after filtering VlnPlot(merged_nc, features = c(&quot;nFeature_RNA&quot;,&quot;nCount_RNA&quot;,&quot;percent.mt&quot;, &quot;percent.rb&quot;), ncol = 4, pt.size = .1) # Normalising data, this is done to account for sequencing depth. Conventional way of doing is to scale it to 10000, and use log2 on the obtained values. merged_nc &lt;- NormalizeData(merged_nc, normalization.method = &quot;LogNormalize&quot;, scale.factor = 10000) # Selecting highly variable features, selection method is variance stabilising transformation merged_nc &lt;- FindVariableFeatures(merged_nc, selection.method = &quot;vst&quot;, nfeatures = 2000) # Selecting top 10 most highly variable genes (most interesting for downstream analysis) top10 &lt;- head(VariableFeatures(merged_nc),10) # Plotting highly variable features (top 10 with labels) vfplot &lt;- VariableFeaturePlot(merged_nc) LabelPoints(plot = vfplot, points = top10, repel = TRUE, xnudge = 0, ynudge = 0) 6.4 Credits Sanbomic’s Cell ranger explanation scRNA-seq analyzing course Single Cell Genomics, Transcriptomics &amp; Proteomics channel "],["sql-and-relational-database-in-dbeaver.html", "7 SQL and relational database in Dbeaver 7.1 Summary of showcased skills", " 7 SQL and relational database in Dbeaver In this report, the focus is on showcasing my skills in SQL. I have used the database management tool Dbeaver to save my tables in. library(readr) library(stringr) library(tidyverse) library(dslabs) library(RPostgreSQL) library(png) library(grid) library(gridExtra) library(ggpubr) library(here) In the chunk of R code below, I have loaded in the data from github. I have transformed the data into tidy format and made variables coincide with eachtother to merge them later on. # Load data flu_df &lt;- read.csv(file = &quot;https://raw.githubusercontent.com/DataScienceILC/tlsc-dsfb26v-20_workflows/main/data/flu_data.csv&quot;, skip = 11) dengue_df &lt;- read.csv(file = &quot;https://raw.githubusercontent.com/DataScienceILC/tlsc-dsfb26v-20_workflows/main/data/dengue_data.csv&quot;, skip = 11) # Transform data to tidy form and make variables country and date coincide with eachother flu_df_tidy &lt;- flu_df %&gt;% pivot_longer(cols = -Date, names_to = &quot;country&quot;, values_to = &quot;flu_count&quot;) %&gt;% na.omit() dengue_df_tidy &lt;- dengue_df %&gt;% pivot_longer(cols = -Date, names_to = &quot;country&quot;, values_to = &quot;dengue_count&quot;) %&gt;% na.omit() dengue_df_tidy$dengue_count &lt;- dengue_df_tidy$dengue_count * 1000 dengue_df_tidy$dengue_count &lt;- as.integer(dengue_df_tidy$dengue_count) dengue_df_tidy$country &lt;- as.factor(dengue_df_tidy$country) flu_df_tidy$country &lt;- as.factor(flu_df_tidy$country) flu_df_tidy$Date &lt;- str_sub(flu_df_tidy$Date, start = 1, end = 4) dengue_df_tidy$Date &lt;- str_sub(dengue_df_tidy$Date, start = 1, end = 4) colnames(flu_df_tidy)[1] &lt;- &quot;year&quot; colnames(dengue_df_tidy)[1] &lt;- &quot;year&quot; flu_df_tidy$year &lt;- as.integer(flu_df_tidy$year) dengue_df_tidy$year &lt;- as.integer(dengue_df_tidy$year) # Inspecting the data using the dplyr package head(flu_df_tidy) ## # A tibble: 6 × 3 ## year country flu_count ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; ## 1 2002 Brazil 174 ## 2 2002 Peru 329 ## 3 2003 Brazil 162 ## 4 2003 Peru 315 ## 5 2003 Brazil 174 ## 6 2003 Chile 1 head(dengue_df_tidy) ## # A tibble: 6 × 3 ## year country dengue_count ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; ## 1 2002 Bolivia 101 ## 2 2002 Brazil 73 ## 3 2002 India 62 ## 4 2002 Indonesia 101 ## 5 2002 Singapore 59 ## 6 2003 Bolivia 143 # saving the 3 dataframes as rds and csv files saveRDS(flu_df_tidy, file = here(&quot;Data/flu.rds&quot;)) saveRDS(dengue_df_tidy, file = here(&quot;Data/dengue.rds&quot;)) saveRDS(gapminder, file = here(&quot;Data/gapminder.rds&quot;)) write.csv(flu_df_tidy, file = here(&quot;Data/flu.csv&quot;), row.names = F) write.csv(dengue_df_tidy, file = here(&quot;Data/dengue.csv&quot;), row.names = F) write.csv(gapminder, file = here(&quot;Data/gapminder.csv&quot;), row.names = F) In the chunk below I have connected to a newly created database named workflowsdb and written my dataframes from R to it. # I made a database named &quot;workflowsdb&quot; before connecting to workflowsdb, by putting the following code into the SQL console in Dbeaver: CREATE DATABASE workflowsdb # Connect to database con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;workflowsdb&quot;, host=&quot;localhost&quot;, port=&quot;5432&quot;, user=&quot;postgres&quot;, password= params$password) #SQL scripts for writing tables to database from dataframes. dbWriteTable(con, &quot;flu_table&quot;, flu_df_tidy, overwrite = T) dbWriteTable(con, &quot;dengue_table&quot;, dengue_df_tidy, overwrite = T) dbWriteTable(con, &quot;gapminder_table&quot;, gapminder, overwrite = T) In the following 3 chunks I inspected the data in the tables that are in database workflowsdb located in Dbeaver. SELECT * FROM flu_table (#tab:viewing contents flu_table)Displaying records 1 - 10 year country flu_count 2002 Brazil 174 2002 Peru 329 2003 Brazil 162 2003 Peru 315 2003 Brazil 174 2003 Chile 1 2003 Peru 314 2003 Brazil 162 2003 Chile 0 2003 Peru 267 SELECT * FROM dengue_table (#tab:viewing contents dengue_table)Displaying records 1 - 10 year country dengue_count 2002 Bolivia 101 2002 Brazil 73 2002 India 62 2002 Indonesia 101 2002 Singapore 59 2003 Bolivia 143 2003 Brazil 98 2003 India 47 2003 Indonesia 39 2003 Singapore 59 SELECT * FROM gapminder_table (#tab:viewing contents gapminder_table)Displaying records 1 - 10 country year infant_mortality life_expectancy fertility population gdp continent region Albania 1960 115.40 62.87 6.19 1636054 NA Europe Southern Europe Algeria 1960 148.20 47.50 7.65 11124892 13828152297 Africa Northern Africa Angola 1960 208.00 35.98 7.32 5270844 NA Africa Middle Africa Antigua and Barbuda 1960 NA 62.97 4.43 54681 NA Americas Caribbean Argentina 1960 59.87 65.39 3.11 20619075 108322326649 Americas South America Armenia 1960 NA 66.86 4.55 1867396 NA Asia Western Asia Aruba 1960 NA 65.66 4.82 54208 NA Americas Caribbean Australia 1960 20.30 70.87 3.45 10292328 96677859364 Oceania Australia and New Zealand Austria 1960 37.30 68.75 2.70 7065525 52392699681 Europe Western Europe Azerbaijan 1960 NA 61.33 5.57 3897889 NA Asia Western Asia Below, a picture of a saved file containing the SQL scripts written above for inspecting the tables’ data. saving_scripts_img &lt;- rasterGrob(as.raster(readPNG(here(&quot;Figures/scripts_saved.png&quot;)))) grid.arrange(saving_scripts_img) In the SQL script below, I have merged the three tables together into one table named merged_table with LEFT JOIN. CREATE TABLE merged_table AS SELECT gapminder_table.year, gapminder_table.country, gapminder_table.life_expectancy, gapminder_table.infant_mortality, gapminder_table.fertility, gapminder_table.population, gapminder_table.gdp, flu_table.flu_count, dengue_table.dengue_count FROM gapminder_table LEFT JOIN flu_table ON gapminder_table.country = flu_table.country AND gapminder_table.year = flu_table.year LEFT JOIN dengue_table ON gapminder_table.country = dengue_table.country AND gapminder_table.year = dengue_table.year The merged table that I made in the chunk above, I’ve imported into R and made a couple of simple plots with ggplot(). # Reading data merged_table &lt;- read.csv(file = here(&quot;Data/merged_table_202305222202.csv&quot;)) merged_table$country &lt;- as.factor(merged_table$country) # Taking only data from countries Bolivia, Mexico and Brazil from years 2010-2015 merged_table_3 &lt;- merged_table %&gt;% filter(country == &quot;Bolivia&quot; | country == &quot;Mexico&quot; | country == &quot;Brazil&quot;, year %in% c(2010:2015)) # How many flu cases Bolivia, Mexico and Brazil have had in total from 1960-2016 # Bolivia&#39;s flu cases merged_table %&gt;% filter(country == &quot;Bolivia&quot;) %&gt;% summarise(sum=sum(flu_count, na.rm = T)) %&gt;% .$sum ## [1] 9896067 # Mexico&#39;s flu cases merged_table %&gt;% filter(country == &quot;Mexico&quot;) %&gt;% summarise(sum=sum(flu_count, na.rm = T)) %&gt;% .$sum ## [1] 35813044 # Brazil&#39;s flu cases merged_table %&gt;% filter(country == &quot;Brazil&quot;) %&gt;% summarise(sum=sum(flu_count, na.rm = T)) %&gt;% .$sum ## [1] 7099615 plotting_merged &lt;- merged_table_3 %&gt;% group_by(year, country) %&gt;% summarise(flu_cases=sum(flu_count, na.rm = T), dengue_cases = sum(dengue_count, na.rm = T)) # Plotting flu cases for three countries from 2010-2015 flu_plot &lt;- plotting_merged %&gt;% ggplot(aes(x = year, y = flu_cases, colour = country)) + geom_point() + geom_smooth()+ theme_bw() + labs(title = &quot;Flu cases Bolivia, Brazil and Mexico \\nfrom 2010-2015&quot;, x = &quot;Year&quot;, y = &quot;Flu cases&quot;) # Plotting dengue cases for three countries from 2010-2015 dengue_plot &lt;- plotting_merged %&gt;% ggplot(aes(x = year, y = dengue_cases, colour = country)) + geom_point() + geom_smooth()+ theme_bw() + labs(title = &quot;Dengue cases Bolivia, Brazil \\nand Mexico from 2010-2015&quot;, x = &quot;Year&quot;, y = &quot;Dengue cases&quot;) # Filtering for the three countries, without filtering for years countries_3 &lt;- merged_table %&gt;% filter(country == &quot;Bolivia&quot; | country == &quot;Mexico&quot; | country == &quot;Brazil&quot;) # obtaining Pearson&#39;s cor coefficient between infant mortality and fertility cor_coefficient &lt;- round(cor.test(countries_3$infant_mortality, countries_3$fertility, method = c(&quot;pearson&quot;))$estimate,2) relations_plot &lt;- countries_3 %&gt;% ggplot(aes(x = infant_mortality, y = fertility, colour = country)) + geom_point() + theme_bw() + annotate(&quot;text&quot;, x = 140, y = 4, size=4, label = paste(&quot;Pearson&#39;s r = &quot;, cor_coefficient)) + labs(title = &quot;Relation between fertility and infant mortality&quot;, x = &quot;Infant mortality&quot;, y = &quot;Fertility&quot;) # Arranging the plots ggarrange(flu_plot,dengue_plot,relations_plot, common.legend = T, labels = &quot;AUTO&quot;) In plot A and B I have plotted the flu- and dengue cases in Bolivia, Mexico and Brazil respectively from 2010-2015. In Plot C I have plotted the relation between fertility and infant mortality in these countries. The Pearson’s correlation coefficient is quite high here, it is 0.95 7.1 Summary of showcased skills General SQL skills (creating tables, creating databases, joining tables and inspecting data in tables) Manipulating multiple tables into tidy format and merging them. Saving csv and rds files in R Importing/exporting tables from or to Dbeaver/R Plotting with ggplot function "],["parameterized-covid-19-report.html", "8 Parameterized COVID-19 report", " 8 Parameterized COVID-19 report This is a report with three parameters. Parameters are country, year and month. After knitting this Rmarkdown with parameters two plots are generated, one with covid deaths, and one with covid cases in specified country, year and month. We start with loading the required libraries library(tidyverse) library(readr) library(stringr) library(tibble) Reading the data from opendata.ecdc.europa.eu, and processing it. # Reading data data &lt;- read.csv(&quot;https://opendata.ecdc.europa.eu/covid19/nationalcasedeath_eueea_daily_ei/csv&quot;, na.strings = &quot;&quot;, fileEncoding = &quot;UTF-8-BOM&quot;) # Tibble for converting monthnumber to monthname months &lt;- tibble(nr = c(1:12), name = c(&quot;January&quot;, &quot;February&quot;, &quot;March&quot;, &quot;April&quot;, &quot;May&quot;, &quot;June&quot;, &quot;July&quot;, &quot;August&quot;, &quot;September&quot;, &quot;October&quot;, &quot;November&quot;, &quot;December&quot;)) # Conversion monthnumber to name month &lt;- months[params$month, &quot;name&quot;] %&gt;% as.character() # Filtering data for specified country, year and month in parameters. country &lt;- data %&gt;% filter(countriesAndTerritories == params$country , year == as.integer(params$year) , month == params$month) # Formatting column dateRep as a date. country$dateRep &lt;- as.Date(country$dateRep, format = &quot;%d/%m/%Y&quot;) In chunk below two plots are visible. One for COVID-19 cases in Belgium in March of 2021. And one for COVID-19 deaths, country %&gt;% ggplot(aes(x = dateRep, y = cases)) + geom_point() + geom_line() + theme_bw() + labs(title = paste0(params$country, &quot; COVID-19 cases in &quot;,month, &quot; of &quot;, params$year), x = &quot;Date&quot;, y = &quot;Cases&quot;) country %&gt;% ggplot(aes(x = dateRep, y = deaths)) + geom_point() + geom_line() + theme_bw() + labs(title = paste0(params$country, &quot; COVID-19 deaths in &quot;,month, &quot; of &quot;, params$year), x = &quot;Date&quot;, y = &quot;Deaths&quot;) "],["simplefunctions-r-package.html", "9 simplefunctions R package 9.1 Installation 9.2 Function 1: get_date() 9.3 Function 2: disease_count() 9.4 Function 3: plot_cases() 9.5 Function 4: load_image()", " 9 simplefunctions R package Simplefunctions is an R package consisting of four functions that I have made. The functions are really basic and quite specific. The functions are: Function 1: get_date() Function 2: disease_count() Function 3: plot_cases() Function 4: load_image() These functions are explained with examples here on my github page. I will also give short descriptions of what these functions do. The package also comes with 2 datasets named flu_cases and dengue_cases. With flu_cases showing the estimated flu cases for different countries per year, and dengue_cases showing the same thing, but for estimated dengue cases. Use ?flu_cases and ?dengue_cases after installation for the documentation of these datasets. 9.1 Installation You can install the development version of simplefunctions from GitHub with: # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;auyar01/simplefunctions&quot;, build_vignettes = TRUE) 9.2 Function 1: get_date() When a dataframe contains a column with dates in yyyy/mm/dd format, this function can be used to get only the years, months or days. I have used the code of this function to get the years for my datasets (dengue_cases and flu_cases). 9.3 Function 2: disease_count() This function counts the total amount of cases of dengue or flu in a country throughout the years. This function is fairly specific for the 2 datasets in this package. 9.4 Function 3: plot_cases() This function plots a line graph of dengue/flu cases throughout the years in different countries. It uses all the countries in the dataframe, so it is best used after filtering for only the countries you want. 9.5 Function 4: load_image() This function is not really related to previous functions. This one’s just for loading an image by giving the path to the file. "],["zotero-for-references.html", "10 Zotero for references 10.1 Introduction to projecticum 10.2 References", " 10 Zotero for references 10.1 Introduction to projecticum One of the topics Nutricia Research is interested in tackling is sarcopenia. Sarcopenia is the progressive loss of muscle mass, and is often witnessed in older individuals This poses a problem, because there is a significant correlation between the loss of muscle mass and negative outcomes like prolonged hospital stays, increased risk of infections and noninfectious complications, and higher overall mortality rates. (Boirie and Guillet 2018) Sarcopenia has multiple contributing factors. Some of these factors include reduced physical activity, heightened oxidative stress, an inflammatory state, changes in hormonal levels, resistance to muscle-building processes, and inadequate nutrition. (Van Dijk et al. 2016) Older adults require a higher amount of protein in their diet compared to younger ones. This is important for their overall health, aid in recovery from illness, and maintain their physical abilities. As people age, their bodies undergo changes in protein metabolism. To help these older individuals (those that are older than 65 years) maintain body mass and function, it is recommended that their average daily protein per kilogram of body weight per day ranges between 1.0 to 1.2. It is also recommended that older individuals undergo both endurance- and resistance-type exercises at levels that are safe and tolerated. And the one’s that are exercising at a more frequent level should have a protein intake of around 1.2 grams per kilogram body weight per day. (Bauer et al. 2013) Leucine is one of the most important amino acid for protein synthesis. It is a nutritionally essential branched-chain amino acid (BCAA). Leucine is one of the most abundant amine acids in high quality protein foods. By activating the mammalian target of rapamycin (mTOR) signaling in skeletal muscle, adipose tissue and placental cells, leucine increases protein synthesis. Leucine also promotes metabolism of energy (by uptake of glucose, mitochondrial biogenesis and fatty acid oxidation) in order to fulfill the energy needs that are needed for protein synthesis. Furthermore, leucine plays a part in inhibiting protein degradation. Approximately 20% of Leucine is converted to alfa-ketoisocaproate (alfa-KIC) and beta-hydroxy-beta-methylbutyrate (HMB) in skeletal muscle. It has been hypothesized that the functions of these metabolites are responsible for the effects of Leucine on the muscles. That is why these metabolites have also received considerable attention as nutritional supplements. (Duan et al. 2016) Nutricia Research is aware of the effects that Leucine provides in regards to protein synthesis and the inhibition of protein degradation. This is why Nutricia Research is experimenting with adding Leucine to plant based diets to make these diets more viable for protein synthesis. For the projecticum, me and 2 classmates are going to be visualising a complex dataset of amino acid concentrations in mice (plasma) put on different diets. The difficulty of this project stems from the fact that for every experimental group (5 groups) there are multiple mice, where each mice have 21 concentrations of amino acids in plasma. The negative control in this experiment is the group fasted (F) where the mice fasted and amino acids concentrations were measured. The positive control for this experiment is whey (W). These groups are shown in this paper (Dijk et al. 2018) 10.2 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
